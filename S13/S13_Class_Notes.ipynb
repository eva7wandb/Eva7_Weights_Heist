{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S13-Class Notes.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eva7wandb/Eva7_Weights_Heist/blob/main/S13/S13_Class_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OgNk8XzlthK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections.abc\n",
        "\n",
        "def to_2tuple(x):\n",
        "  if isinstance(x, collections.abc.Iterable):\n",
        "    return x\n",
        "  return x, x\n",
        "\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "\n",
        "img_size = to_2tuple(img_size)\n",
        "patch_size = to_2tuple(patch_size)\n",
        "\n",
        "img_size, patch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ahDvIabAQyi",
        "outputId": "55b0bd2a-8b34-4c4b-c980-3bce688b7b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((224, 224), (16, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV_dhhiEAjvv",
        "outputId": "7fb8b7e0-e671-4a83-96d6-2aece8a37da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proj = nn.Conv2d(3, 768, 16, 16)\n",
        "y = proj(x)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCFtg_45Am0R",
        "outputId": "0f0aff39-881f-4328-9c2b-9985bc819841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 14, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.flatten(2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUMqKml2AwyD",
        "outputId": "d6bc18ab-6801-440d-aeb9-57d81495f4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 196])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.flatten(2).transpose(1, 2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwKYc3a4Azwr",
        "outputId": "8ae08c77-8c62-43a6-8c4d-776132a676f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        image_size = to_2tuple(image_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_channels, height, width = pixel_values.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        if height != self.image_size[0] or width != self.image_size[1]:\n",
        "            raise ValueError(\n",
        "                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n",
        "            )\n",
        "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nyCNxvx5A2Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token = nn.Parameter(torch.zeros(1, 1, 768))\n",
        "cls_token.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCCkViMVBJfi",
        "outputId": "1975fb9d-9029-4b16-8363-032c9de38ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token.expand(32, -1, -1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcFqUP_8BMon",
        "outputId": "be9abb54-db43-487d-f38d-0612dd71abdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_embedding = nn.Parameter(torch.zeros(1, 14*14 + 1, 768))\n",
        "position_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdWMPRbDBPQA",
        "outputId": "e8021cfe-e8e5-4c24-dd34-01c6c0d347fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct the CLS token, position and patch embeddings.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "        self.patch_embeddings = PatchEmbeddings(\n",
        "            image_size=config.image_size,\n",
        "            patch_size=config.patch_size,\n",
        "            num_channels=config.num_channels,\n",
        "            embed_dim=config.hidden_size,\n",
        "        )\n",
        "        num_patches = self.patch_embeddings.num_patches\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "hS3u4DAaBSoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTConfig():\n",
        "  def __init__(\n",
        "        self,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        is_encoder_decoder=False,\n",
        "        image_size=224,\n",
        "        patch_size=16,\n",
        "        num_channels=3,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_act = hidden_act\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "\n",
        "configuration = ViTConfig()\n",
        "# You can read full configuration file here: https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit/configuration_vit.py"
      ],
      "metadata": {
        "id": "B1L3vAmLBv2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars(configuration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkByMEqGB9FJ",
        "outputId": "a5345dc6-729d-4b2c-81cc-3ed3f6e6bb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.0,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.0,\n",
              " 'hidden_size': 768,\n",
              " 'image_size': 224,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'layer_norm_eps': 1e-12,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_channels': 3,\n",
              " 'num_hidden_layers': 12,\n",
              " 'patch_size': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "vit_emb = ViTEmbeddings(configuration)\n",
        "vit_emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1cp2fQQB_kT",
        "outputId": "53fc64e6-8a16-460e-9634-8fd73151375a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEmbeddings(\n",
              "  (patch_embeddings): PatchEmbeddings(\n",
              "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = vit_emb(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zEvSY0-CHQL",
        "outputId": "f7885c68-2602-494d-ae1b-42d58a63138d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat = nn.Linear(768, 12*64)\n",
        "mat = mat(out)\n",
        "mat.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaiwdORICUo8",
        "outputId": "1a0f9dd8-adb4-4038-b38f-ff170f6bce9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat.size()[:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj-QJaALCm6x",
        "outputId": "f0ea65d9-e9ce-49a3-d5f9-396c0f24b87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_shape = mat.size()[:-1] + (12, 64)\n",
        "new_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IsBSHMECpm_",
        "outputId": "855e11c0-18ee-4ff6-f59d-daea5e12e3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 12, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)\n",
        "out = out.view(*new_shape)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZlXJmJOCv3I",
        "outputId": "6449831a-738a-4bfd-ccd6-69cf8d87b700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 12, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = out.permute(0, 2, 1, 3)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAvYtCFxDC62",
        "outputId": "c65ee30b-2201-4d1b-831c-176cb35ac770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 12, 197, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = out"
      ],
      "metadata": {
        "id": "-4br8K2UDMDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(out, out2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "4W0c_BSsDS3X",
        "outputId": "eaec63c9-a4e1-4b24-e5d3-874dc9838e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-db20b78eb9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape, out2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Hu59viDYPY",
        "outputId": "a5e8d53a-ba05-4cfd-ebf1-153244e360ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 12, 197, 64]), torch.Size([32, 12, 197, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out2.transpose(-1, -2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWgJqWZ3DjII",
        "outputId": "dc7bf8ae-da69-4607-a180-a00a4a0e87e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 64, 197])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores = torch.matmul(out, out2.transpose(-1, -2))\n",
        "attention_scores.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Gz0eawDn-9",
        "outputId": "cf3a8262-f85f-4b5f-8709-8ae0b8841d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 197, 197])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer = torch.matmul(nn.Softmax(dim=-1)(attention_scores), out)\n",
        "context_layer.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hzZ26huDqiw",
        "outputId": "53fb4c64-1f69-457e-8fcd-25379e5ec64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 197, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer = context_layer.permute(0, 2, 1, 3)\n",
        "context_layer.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlRw20fqDwRI",
        "outputId": "9f6fc4fc-563b-42eb-9d76-7d68e481e76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 12, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer.size()[:-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQmfLofnD8Ul",
        "outputId": "4c34bb91-355a-433d-e382-fb9c2bf028f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_context_layer_shape = context_layer.size()[:-2] + (12*64,)\n",
        "new_context_layer_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InIuL6qQEA4A",
        "outputId": "8809df15-e02f-4183-d4d7-186b5f0d7280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(context_layer.shape)\n",
        "context_layer.view(*new_context_layer_shape)\n",
        "print(context_layer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "ckQUcy9BEC64",
        "outputId": "676ad000-6191-4aec-e40c-8333f21e99fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 197, 12, 64])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-67dc3e434ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer = torch.matmul(nn.Softmax(dim=-1)(attention_scores), out)\n",
        "print(context_layer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb8n9ru1EFUZ",
        "outputId": "55e00969-f881-47a0-eedc-0fb26c325b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 12, 197, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()"
      ],
      "metadata": {
        "id": "4x8-vpqSEpiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(context_layer.shape)\n",
        "print(context_layer.size()[:-2])\n",
        "new_context_layer_shape = context_layer.size()[:-2] + (12*64,)\n",
        "print(new_context_layer_shape)\n",
        "print(context_layer.shape)\n",
        "context_layer.view(*new_context_layer_shape)\n",
        "print(context_layer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlAVwkNDEt3k",
        "outputId": "574edb92-e61f-460b-ceb5-219d5192f007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 197, 12, 64])\n",
            "torch.Size([32, 197])\n",
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 12, 64])\n",
            "torch.Size([32, 197, 12, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 2)\n",
        "y = torch.transpose(x, 0, 1)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jAyEdzeEv3n",
        "outputId": "0e914e9c-9408-4d51-f0b4-e4ad61d55cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3147, -1.1145],\n",
            "        [-1.0985, -1.1511],\n",
            "        [-0.0234, -0.0995]])\n",
            "tensor([[ 1.3147, -1.0985, -0.0234],\n",
            "        [-1.1145, -1.1511, -0.0995]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0, 1] = 42\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.is_contiguous())\n",
        "print(y.is_contiguous())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yy_cvELE0zE",
        "outputId": "a2f045f1-5fe5-4059-f74f-a0d58790e71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3147e+00,  4.2000e+01],\n",
            "        [-1.0985e+00, -1.1511e+00],\n",
            "        [-2.3425e-02, -9.9486e-02]])\n",
            "tensor([[ 1.3147e+00, -1.0985e+00, -2.3425e-02],\n",
            "        [ 4.2000e+01, -1.1511e+00, -9.9486e-02]])\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class ViTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
        "                f\"heads {config.num_attention_heads}.\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs\n",
        "        "
      ],
      "metadata": {
        "id": "bpNKMkwuE4b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_atn = ViTSelfAttention(configuration)\n",
        "vit_atn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOASjewKFoFR",
        "outputId": "deef5a49-4f62-4116-f936-ca5f79656298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTSelfAttention(\n",
              "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "vit_emb = ViTEmbeddings(configuration)\n",
        "vit_emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS6YOIEfFpbC",
        "outputId": "d5a71b39-598f-4d2f-9502-66ec30f89eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEmbeddings(\n",
              "  (patch_embeddings): PatchEmbeddings(\n",
              "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = vit_emb(x)\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DwHOnXTFszm",
        "outputId": "22e23774-6592-4b68-f03e-51c806c0f509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer, attention_probs = vit_atn(emb, head_mask=None, output_attentions=True)"
      ],
      "metadata": {
        "id": "gM1fG58bFujY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_layer.shape, attention_probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa_WVzAiFyzJ",
        "outputId": "fc2e698b-4b5f-4158-d162-10111fa49e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 197, 768]), torch.Size([32, 12, 197, 197]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTSelfOutput(nn.Module):\n",
        "  \"\"\"\n",
        "  This is just a Linear Layer Block\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "    return hidden_states"
      ],
      "metadata": {
        "id": "ObTomYChF0Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = ViTSelfAttention(config)\n",
        "        self.output = ViTSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.attention.query = prune_linear_layer(self.attention.query, index)\n",
        "        self.attention.key = prune_linear_layer(self.attention.key, index)\n",
        "        self.attention.value = prune_linear_layer(self.attention.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n",
        "        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
        "\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "G6N-cQXZF5m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = nn.functional.gelu(hidden_states)\n",
        "\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "9rm48whnGJE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states + input_tensor\n",
        "\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "Rb47l2NgGOcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = ViTAttention(config)\n",
        "        self.intermediate = ViTIntermediate(config)\n",
        "        self.output = ViTOutput(config)\n",
        "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        # first residual connection\n",
        "        hidden_states = attention_output + hidden_states\n",
        "\n",
        "        # in ViT, layernorm is also applied after self-attention\n",
        "        layer_output = self.layernorm_after(hidden_states)\n",
        "\n",
        "        layer_output = self.intermediate(layer_output)\n",
        "\n",
        "        # second residual connection is done here\n",
        "        layer_output = self.output(layer_output, hidden_states)\n",
        "\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output)\n",
        "        return layer_output"
      ],
      "metadata": {
        "id": "E4szs9vJGQAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    layer_head_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        \n",
        "        return hidden_states,all_hidden_states,all_self_attentions\n"
      ],
      "metadata": {
        "id": "tWfHk0VIGYqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTModel():\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = ViTEmbeddings(config)\n",
        "        self.encoder = ViTEncoder(config)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.patch_embeddings\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Returns:\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            >>> from transformers import ViTFeatureExtractor, ViTModel\n",
        "            >>> from PIL import Image\n",
        "            >>> import requests\n",
        "\n",
        "            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "            >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "            >>> feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "            >>> model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> last_hidden_states = outputs.last_hidden_state\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(pixel_values)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return sequence_output,pooled_output,encoder_outputs.hidden_states,encoder_outputs.attentions\n"
      ],
      "metadata": {
        "id": "7_sJwaPxGdDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ],
      "metadata": {
        "id": "-tsm7b6GGlJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape, configuration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9UlVgLLGnbU",
        "outputId": "782d8ecf-a4ef-468c-c3fd-2d8b3d0be8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 12, 197, 64]), <__main__.ViTConfig at 0x7f785df6ee90>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_enc = ViTEncoder(configuration)"
      ],
      "metadata": {
        "id": "KykWc0OIGpWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_enc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkvh5OR3Grcm",
        "outputId": "9800ab41-cf6a-41ae-a5b0-edb5cfc619b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEncoder(\n",
              "  (layer): ModuleList(\n",
              "    (0): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (1): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (2): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (3): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (4): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (5): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (6): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (7): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (8): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (9): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (10): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (11): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.rand((32, 3, 224, 224))\n",
        "embeddings = ViTEmbeddings(configuration)\n",
        "encoder = ViTEncoder(configuration)\n",
        "layernorm = nn.LayerNorm(configuration.hidden_size, eps=0.000001)"
      ],
      "metadata": {
        "id": "oTpqc9CAGs3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_output = embeddings(input)\n",
        "embedding_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TivcbKbG0kW",
        "outputId": "38a54a9e-8352-4d40-ccf4-d48f004a6642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output = encoder(embedding_output)"
      ],
      "metadata": {
        "id": "OHU67F2dG1lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(encoder_output), len(encoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZG78SHnG2xj",
        "outputId": "22d4b235-ae3a-44c5-dc8f-9ec06fbeb845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tuple, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states, all_hidden_states, all_self_attentions = encoder_output\n",
        "\n",
        "hidden_states.shape, all_hidden_states, all_self_attentions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcrf-_RqG4I6",
        "outputId": "d1f9bbbe-2e40-490f-8550-299f5aafde2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 197, 768]), None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_output = encoder_output[0]\n",
        "sequence_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv_HXAusG9Cp",
        "outputId": "200c1732-5c1c-40f0-eac0-e96a2868df4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_output[:, 0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xju5aq8eG-Y9",
        "outputId": "fb31a6d7-3a34-4e5b-e839-ba8811d60202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_output = encoder_output[0]\n",
        "layernorm = nn.LayerNorm(configuration.hidden_size, eps=0.00001)\n",
        "sequence_output = layernorm(sequence_output)\n",
        "# VitPooler\n",
        "dense = nn.Linear(configuration.hidden_size, configuration.hidden_size)\n",
        "activation = nn.Tanh()\n",
        "first_token_tensor = sequence_output[:, 0]\n",
        "pooled_output = dense(first_token_tensor)\n",
        "pooled_output = activation(pooled_output)\n",
        "pooled_output.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCkb5_kCG_6Z",
        "outputId": "32b4b879-120a-49e8-cd03-49a547a35af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = nn.Linear(configuration.hidden_size, 100)\n",
        "logits = classifier(pooled_output)\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkViWvsYHBHZ",
        "outputId": "75b89230-f54f-470c-ddbd-7e7d998551c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t_Ec3hC4HKEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}